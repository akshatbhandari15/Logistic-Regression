# -*- coding: utf-8 -*-
"""
Created on Fri Dec 18 17:06:01 2020

@author: aksha
"""
import numpy as np
import pandas as pd
train = pd.read_csv("train.csv")
train = train.drop(["PassengerId", "Name", "Ticket", "Cabin", "Age"], axis = 1)
train = train.replace(to_replace="C", value=1)
train = train.replace(to_replace="S", value=2)
train = train.replace(to_replace="Q", value=3)

train = train.replace(to_replace="male", value=1)
train = train.replace(to_replace="female", value=2)
train.insert(0, "X", 1 )


survived = train.Survived
train = train.drop(["Survived"], axis = 1)

train = train.to_numpy()
survived = survived.to_numpy()

no_of_features = train.shape[1]
theta = np.zeros([no_of_features, 1 ])
m = len(survived)

h = train @ theta
s = np.isnan(h)
h[s]=0.0
alpha = 0.1
y = survived[:, np.newaxis]


def sigmoid(z):
  return 1.0 / (1 + np.exp(-z))

def predict(features, weights):

  z = np.dot(features, weights)
  return sigmoid(z)

def cost_function(features, labels, weights):

    observations = len(labels)

    predictions = predict(features, weights)

    class1_cost = -labels*np.log(predictions)

    class2_cost = (1-labels)*np.log(1-predictions)

    cost = class1_cost - class2_cost

    cost = cost.sum() / observations

    return cost

def update_weights(features, labels, weights, lr):

    N = len(features)

    predictions = predict(features, weights)

    gradient = np.dot(features.T,  predictions - labels)

    gradient /= N

    gradient *= lr

    weights = weights - gradient

    return weights


def decision_boundary(prob):
  return 1 if prob >= .5 else 0

def LogReg(features, labels, weights, lr, iters):
    cost_history = []

    for i in range(iters):
        weights = update_weights(features, labels, weights, lr)

        cost = cost_function(features, labels, weights)
        cost_history.append(cost)

        # Log Progress
        if i % 1000 == 0:
            print ("iter: "+str(i) + " cost: "+str(cost))

    return weights, cost_history

LogReg(train, survived, theta, alpha, 1000)













